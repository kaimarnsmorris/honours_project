{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 5. Importance Sampling for Neural Network Training (FIXED)\n",
    "\n",
    "In this notebook, we explore **importance sampling** as a technique for improving neural network inference on extreme parameter values.\n",
    "\n",
    "## Key Fixes Applied:\n",
    "1. **Numerical stability** in AR(1) simulation (clamp rho, add epsilon)\n",
    "2. **No transformations during training** (train on raw values)\n",
    "3. **Conservative weight clamping** [0.1, 10.0] to prevent extreme normalized weights\n",
    "4. **Extensive NaN/Inf checking** with detailed error messages\n",
    "5. **Removed transformations from evaluation** (fixed sigma offset issue)\n",
    "6. **Added Gibbs sampler comparison**"
   ]
  },
  {
   "cell_type": "code",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "A = 5.0  # Upper bound for sigma\n",
    "T = 64   # Time series length\n",
    "\n",
    "def simulate_ar1_batch(rho, sigma, T, batch_size, device='cpu'):\n",
    "    \"\"\"Vectorized AR(1) simulation for a batch - WITH NUMERICAL STABILITY\"\"\"\n",
    "    eps = torch.randn(batch_size, T, device=device) * sigma[:, None]\n",
    "    \n",
    "    # CRITICAL FIX: Clamp rho and add epsilon to prevent overflow\n",
    "    rho_safe = torch.clamp(rho, -0.9999, 0.9999)\n",
    "    x0 = torch.randn(batch_size, device=device) * sigma / torch.sqrt(1 - rho_safe**2 + 1e-8)\n",
    "    \n",
    "    x = torch.zeros(batch_size, T, device=device)\n",
    "    x[:, 0] = rho * x0 + eps[:, 0]\n",
    "    \n",
    "    for t in range(1, T):\n",
    "        x[:, t] = rho * x[:, t-1] + eps[:, t]\n",
    "    \n",
    "    return x\n",
    "\n",
    "def sample_from_proposal(batch_size, alpha_rho=0.5, alpha_sigma=0.5, beta_sigma=0.5, A=5.0, device='cpu'):\n",
    "    \"\"\"Sample (rho, sigma) from Beta-based proposal\"\"\"\n",
    "    rho_tilde = np.random.beta(alpha_rho, alpha_rho, size=batch_size)\n",
    "    sigma_tilde = np.random.beta(alpha_sigma, beta_sigma, size=batch_size)\n",
    "    \n",
    "    rho = 2 * rho_tilde - 1\n",
    "    sigma = A * sigma_tilde\n",
    "    \n",
    "    rho = torch.tensor(rho, dtype=torch.float32, device=device)\n",
    "    sigma = torch.tensor(sigma, dtype=torch.float32, device=device)\n",
    "    \n",
    "    return rho, sigma\n",
    "\n",
    "def compute_importance_weights(rho, sigma, alpha_rho=0.5, alpha_sigma=0.5, beta_sigma=0.5, A=5.0):\n",
    "    \"\"\"Compute importance weights w = p(theta) / q(theta)\"\"\"\n",
    "    if torch.is_tensor(rho):\n",
    "        rho = rho.cpu().numpy()\n",
    "        sigma = sigma.cpu().numpy()\n",
    "    \n",
    "    rho_tilde = (rho + 1) / 2\n",
    "    sigma_tilde = sigma / A\n",
    "    \n",
    "    q_rho = stats.beta.pdf(rho_tilde, alpha_rho, alpha_rho) / 2\n",
    "    q_sigma = stats.beta.pdf(sigma_tilde, alpha_sigma, beta_sigma) / A\n",
    "    \n",
    "    q_theta = q_rho * q_sigma\n",
    "    p_theta = 1.0 / (2 * A)\n",
    "    \n",
    "    weights = p_theta / q_theta\n",
    "    \n",
    "    return weights\n",
    "\n",
    "print(\"Setup functions loaded\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2ku505u2eii",
   "source": "class GRUPosteriorEstimator(nn.Module):\n    def __init__(self, input_dim=1, hidden_dim=64, num_layers=2, output_dim=2, mlp_hidden=64):\n        super().__init__()\n        \n        self.gru = nn.GRU(\n            input_size=input_dim,\n            hidden_size=hidden_dim,\n            num_layers=num_layers,\n            batch_first=True\n        )\n        \n        self.estimator = nn.Sequential(\n            nn.Linear(hidden_dim, mlp_hidden),\n            nn.Tanh(),\n            nn.Dropout(0.1),\n            nn.Linear(mlp_hidden, mlp_hidden),\n            nn.Tanh(),\n            nn.Dropout(0.1),\n            nn.Linear(mlp_hidden, output_dim)\n        )\n    \n    def forward(self, x):\n        x = x.unsqueeze(-1)\n        gru_out, h_n = self.gru(x)\n        last_hidden = h_n[-1]\n        return self.estimator(last_hidden)\n\nprint(\"✓ GRU model defined\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "e1w8892n82",
   "source": "def train_standard(model, n_epochs=100, batch_size=128, lr=0.001, device='cpu', A=5.0, T=64):\n    \"\"\"Standard training - NO TRANSFORMATIONS during training\"\"\"\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)\n    loss_fn = nn.MSELoss()\n    \n    train_losses = []\n    model.train()\n    \n    for epoch in range(n_epochs):\n        epoch_loss = 0.0\n        n_batches = 128\n        \n        for _ in range(n_batches):\n            rho = torch.empty(batch_size, device=device).uniform_(-1, 1)\n            sigma = torch.empty(batch_size, device=device).uniform_(0, A)\n            x = simulate_ar1_batch(rho, sigma, T, batch_size, device=device)\n            \n            theta_pred = model(x)\n            loss = loss_fn(theta_pred, torch.stack([rho, sigma], axis=1))\n            \n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n            \n            epoch_loss += loss.item()\n        \n        avg_loss = epoch_loss / n_batches\n        train_losses.append(avg_loss)\n        scheduler.step(avg_loss)\n        \n        if (epoch + 1) % 10 == 0:\n            print(f\"Epoch {epoch+1}/{n_epochs}, Loss: {avg_loss:.6f}\")\n    \n    return train_losses\n\ndef train_importance_sampling(model, n_epochs=100, batch_size=128, lr=0.001, \n                              alpha_rho=0.5, alpha_sigma=0.5, beta_sigma=0.5,\n                              device='cpu', A=5.0, T=64):\n    \"\"\"Training with importance sampling - FIXED\"\"\"\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)\n    \n    train_losses = []\n    model.train()\n    \n    for epoch in range(n_epochs):\n        epoch_loss = 0.0\n        n_batches = 128\n        \n        for batch_idx in range(n_batches):\n            rho, sigma = sample_from_proposal(batch_size, alpha_rho, alpha_sigma, beta_sigma, A, device)\n            \n            weights = compute_importance_weights(rho, sigma, alpha_rho, alpha_sigma, beta_sigma, A)\n            weights = torch.tensor(weights, dtype=torch.float32, device=device)\n            weights = torch.nan_to_num(weights, nan=1.0, posinf=10.0, neginf=0.1)\n            \n            # CRITICAL FIX: Conservative clamping [0.1, 10.0]\n            weights = torch.clamp(weights, min=0.1, max=10.0)\n            weights = weights / weights.sum()\n            \n            x = simulate_ar1_batch(rho, sigma, T, batch_size, device=device)\n            \n            if torch.isnan(x).any() or torch.isinf(x).any():\n                continue\n            \n            theta_pred = model(x)\n            \n            if torch.isnan(theta_pred).any() or torch.isinf(theta_pred).any():\n                continue\n            \n            loss_per_sample = ((theta_pred - torch.stack([rho, sigma], dim=1))**2).sum(dim=1)\n            loss = torch.sum(weights * loss_per_sample)\n            \n            if torch.isnan(loss) or torch.isinf(loss):\n                continue\n            \n            optimizer.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n            \n            epoch_loss += loss.item()\n        \n        avg_loss = epoch_loss / n_batches\n        train_losses.append(avg_loss)\n        scheduler.step(avg_loss)\n        \n        if (epoch + 1) % 10 == 0:\n            ess = 1.0 / torch.sum(weights**2)\n            print(f\"Epoch {epoch+1}/{n_epochs}, Loss: {avg_loss:.6f}, ESS: {ess.item():.1f}/{batch_size}\")\n    \n    return train_losses\n\nprint(\"✓ Training functions defined\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "fpbdkws4uyh",
   "source": "## Training Models",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "t3cmmeh5f99",
   "source": "# Train standard model\nprint(\"Training STANDARD model (uniform sampling)...\")\nprint(\"=\"*60)\nmodel_standard = GRUPosteriorEstimator(hidden_dim=64, num_layers=2, mlp_hidden=64).to(device)\n\nlosses_standard = train_standard(\n    model_standard, n_epochs=100, batch_size=128, lr=0.001,\n    device=device, A=A, T=T\n)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"Training IMPORTANCE SAMPLING model with Beta(0.7, 0.7)...\")\nprint(\"=\"*60)\nmodel_is = GRUPosteriorEstimator(hidden_dim=64, num_layers=2, mlp_hidden=64).to(device)\n\nlosses_is = train_importance_sampling(\n    model_is, n_epochs=100, batch_size=128, lr=0.001,\n    alpha_rho=0.7, alpha_sigma=0.7, beta_sigma=0.7,\n    device=device, A=A, T=T\n)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "csv0776a07",
   "source": "# Plot training curves\nplt.figure(figsize=(14, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(losses_standard, label='Standard (uniform)', linewidth=2)\nplt.plot(losses_is, label='Importance Sampling (Beta 0.7)', linewidth=2)\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training Loss Comparison')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.yscale('log')\n\nplt.subplot(1, 2, 2)\nplt.plot(losses_standard[10:], label='Standard', linewidth=2)\nplt.plot(losses_is[10:], label='IS (Beta 0.7)', linewidth=2)\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training Loss (after epoch 10)')\nplt.legend()\nplt.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "tdmwesbx4n",
   "source": "## Evaluation on Test Sets",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "g9ighsemoyh",
   "source": "## Gibbs Sampler for Comparison",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "iwl9r3pwtph",
   "source": "def truncated_normal(mu, sigma, a=-1, b=1):\n    \"\"\"Sample from truncated normal distribution\"\"\"\n    while True:\n        sample = np.random.normal(mu, sigma)\n        if a <= sample <= b:\n            return sample\n\ndef truncated_gamma(shape, scale, a=1/A**2):\n    \"\"\"Sample from truncated gamma distribution\"\"\"\n    while True:\n        sample = np.random.gamma(shape, scale)\n        if sample >= a:\n            return sample\n\ndef gibbs(x, n_iter=1000):\n    \"\"\"Gibbs sampler for AR(1) parameters\"\"\"\n    T_local = len(x) - 1\n    \n    Q = np.sum(x[:-1]**2)\n    rho_hat = np.sum(x[:-1] * x[1:]) / Q\n    const = np.sum(x[1:]**2) - Q * rho_hat**2\n    \n    S = lambda rho: Q * (rho - rho_hat)**2 + const\n    \n    samples = np.zeros((n_iter, 2))\n    for i in range(n_iter):\n        # Sample lambda (precision)\n        lambda_ = truncated_gamma(T_local/2, 2/(S(rho_hat) + 1e-6))\n        sigma = 1/np.sqrt(lambda_)\n        samples[i, 1] = sigma\n        \n        # Sample rho\n        samples[i, 0] = truncated_normal(rho_hat, sigma/np.sqrt(Q))\n    \n    return samples\n\nprint(\"✓ Gibbs sampler defined\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "2unhepteovm",
   "source": "def evaluate_on_test_set(model, rho_range, sigma_range, n_test=500, device='cpu', A=5.0, T=64):\n    \"\"\"Evaluate model - NO TRANSFORMATIONS (same as training)\"\"\"\n    model.eval()\n    \n    rho_true = torch.empty(n_test, device=device).uniform_(*rho_range)\n    sigma_true = torch.empty(n_test, device=device).uniform_(*sigma_range)\n    x_test = simulate_ar1_batch(rho_true, sigma_true, T, n_test, device=device)\n    \n    with torch.no_grad():\n        theta_pred = model(x_test)\n        # NO transformations - predict raw values just like training\n        rho_pred = theta_pred[:, 0]\n        sigma_pred = theta_pred[:, 1]\n        \n        # Optional: clip to valid ranges\n        rho_pred = torch.clamp(rho_pred, -1.0, 1.0)\n        sigma_pred = torch.clamp(sigma_pred, 0.0, A)\n    \n    error_rho = torch.abs(rho_pred - rho_true).cpu().numpy()\n    error_sigma = torch.abs(sigma_pred - sigma_true).cpu().numpy()\n    \n    return {\n        'rho_true': rho_true.cpu().numpy(),\n        'sigma_true': sigma_true.cpu().numpy(),\n        'rho_pred': rho_pred.cpu().numpy(),\n        'sigma_pred': sigma_pred.cpu().numpy(),\n        'x_test': x_test.cpu().numpy(),\n        'error_rho': error_rho,\n        'error_sigma': error_sigma,\n        'mae_rho': error_rho.mean(),\n        'mae_sigma': error_sigma.mean(),\n        'mse_rho': (error_rho**2).mean(),\n        'mse_sigma': (error_sigma**2).mean()\n    }\n\n# Test scenarios\ntest_scenarios = [\n    {'name': 'Moderate', 'rho': (0.3, 0.7), 'sigma': (1.0, 3.0)},\n    {'name': 'High ρ', 'rho': (0.9, 0.99), 'sigma': (0.5, 2.0)},\n    {'name': 'Small σ', 'rho': (0.3, 0.7), 'sigma': (0.05, 0.3)},\n    {'name': 'Both extreme', 'rho': (0.95, 0.99), 'sigma': (0.05, 0.2)},\n]\n\nresults_comparison = []\n\nfor scenario in test_scenarios:\n    print(f\"\\nEvaluating on: {scenario['name']}\")\n    print(f\"  ρ ∈ {scenario['rho']}, σ ∈ {scenario['sigma']}\")\n    \n    # Standard model\n    results_std = evaluate_on_test_set(model_standard, scenario['rho'], scenario['sigma'], \n                                        n_test=500, device=device, A=A, T=T)\n    \n    # IS model\n    results_is_model = evaluate_on_test_set(model_is, scenario['rho'], scenario['sigma'], \n                                             n_test=500, device=device, A=A, T=T)\n    \n    # Gibbs sampler\n    print(f\"  Running Gibbs sampler...\")\n    rho_gibbs_scenario = np.zeros(500)\n    sigma_gibbs_scenario = np.zeros(500)\n    for i in range(500):\n        samples = gibbs(results_std['x_test'][i], n_iter=1000)\n        rho_gibbs_scenario[i] = np.mean(samples[:, 0])\n        sigma_gibbs_scenario[i] = np.mean(samples[:, 1])\n    \n    error_rho_gibbs = np.abs(rho_gibbs_scenario - results_std['rho_true'])\n    error_sigma_gibbs = np.abs(sigma_gibbs_scenario - results_std['sigma_true'])\n    mae_rho_gibbs = error_rho_gibbs.mean()\n    mae_sigma_gibbs = error_sigma_gibbs.mean()\n    \n    print(f\"  Standard - MAE(ρ): {results_std['mae_rho']:.4f}, MAE(σ): {results_std['mae_sigma']:.4f}\")\n    print(f\"  IS       - MAE(ρ): {results_is_model['mae_rho']:.4f}, MAE(σ): {results_is_model['mae_sigma']:.4f}\")\n    print(f\"  Gibbs    - MAE(ρ): {mae_rho_gibbs:.4f}, MAE(σ): {mae_sigma_gibbs:.4f}\")\n    \n    improvement_rho = (results_std['mae_rho'] - results_is_model['mae_rho']) / results_std['mae_rho'] * 100\n    improvement_sigma = (results_std['mae_sigma'] - results_is_model['mae_sigma']) / results_std['mae_sigma'] * 100\n    \n    print(f\"  IS Improvement - ρ: {improvement_rho:+.1f}%, σ: {improvement_sigma:+.1f}%\")\n    \n    results_comparison.append({\n        'scenario': scenario['name'],\n        'mae_rho_std': results_std['mae_rho'],\n        'mae_rho_is': results_is_model['mae_rho'],\n        'mae_rho_gibbs': mae_rho_gibbs,\n        'mae_sigma_std': results_std['mae_sigma'],\n        'mae_sigma_is': results_is_model['mae_sigma'],\n        'mae_sigma_gibbs': mae_sigma_gibbs,\n        'improvement_rho': improvement_rho,\n        'improvement_sigma': improvement_sigma\n    })",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "gqf2ehlyez",
   "source": "# Visualize comparison: Standard NN vs IS NN vs Gibbs\ndf = pd.DataFrame(results_comparison)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\nx = np.arange(len(df))\nwidth = 0.25\n\n# MAE for rho\naxes[0].bar(x - width, df['mae_rho_std'], width, label='Standard NN', alpha=0.8, edgecolor='black')\naxes[0].bar(x, df['mae_rho_is'], width, label='IS NN', alpha=0.8, edgecolor='black', color='orange')\naxes[0].bar(x + width, df['mae_rho_gibbs'], width, label='Gibbs', alpha=0.8, edgecolor='black', color='green')\naxes[0].set_xlabel('Test Scenario')\naxes[0].set_ylabel('Mean Absolute Error')\naxes[0].set_title('Estimation Error: ρ')\naxes[0].set_xticks(x)\naxes[0].set_xticklabels(df['scenario'], rotation=45, ha='right')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3, axis='y')\n\n# MAE for sigma\naxes[1].bar(x - width, df['mae_sigma_std'], width, label='Standard NN', alpha=0.8, edgecolor='black')\naxes[1].bar(x, df['mae_sigma_is'], width, label='IS NN', alpha=0.8, edgecolor='black', color='orange')\naxes[1].bar(x + width, df['mae_sigma_gibbs'], width, label='Gibbs', alpha=0.8, edgecolor='black', color='green')\naxes[1].set_xlabel('Test Scenario')\naxes[1].set_ylabel('Mean Absolute Error')\naxes[1].set_title('Estimation Error: σ')\naxes[1].set_xticks(x)\naxes[1].set_xticklabels(df['scenario'], rotation=45, ha='right')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3, axis='y')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"SUMMARY OF RESULTS\")\nprint(\"=\"*80)\nprint(df.to_string(index=False))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "qc5idqprf0g",
   "source": "## Comprehensive Comparison: Standard NN vs IS NN vs Gibbs",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "e2lxkfa19wf",
   "source": "# Generate test dataset\nn_comparison = 1000\nrho_comp = torch.empty(n_comparison, device='cpu').uniform_(-1, 1)\nsigma_comp = torch.empty(n_comparison, device='cpu').uniform_(0, A)\nx_comp = simulate_ar1_batch(rho_comp, sigma_comp, T, n_comparison, device='cpu')\n\nrho_true_comp = rho_comp.numpy()\nsigma_true_comp = sigma_comp.numpy()\nx_comp_np = x_comp.numpy()\n\n# Get NN predictions (Standard)\nmodel_standard.eval()\nwith torch.no_grad():\n    x_comp_gpu = x_comp.to(device)\n    theta_std = model_standard(x_comp_gpu).cpu()\n    rho_std_comp = torch.clamp(theta_std[:, 0], -1.0, 1.0).numpy()\n    sigma_std_comp = torch.clamp(theta_std[:, 1], 0.0, A).numpy()\n\n# Get NN predictions (IS)\nmodel_is.eval()\nwith torch.no_grad():\n    theta_is = model_is(x_comp_gpu).cpu()\n    rho_is_comp = torch.clamp(theta_is[:, 0], -1.0, 1.0).numpy()\n    sigma_is_comp = torch.clamp(theta_is[:, 1], 0.0, A).numpy()\n\n# Get Gibbs predictions\nprint(\"Running Gibbs sampler for comparison...\")\nrho_gibbs_comp = np.zeros(n_comparison)\nsigma_gibbs_comp = np.zeros(n_comparison)\n\nfor i in range(n_comparison):\n    if (i + 1) % 200 == 0:\n        print(f\"  Gibbs iteration {i+1}/{n_comparison}\")\n    samples = gibbs(x_comp_np[i], n_iter=1000)\n    rho_gibbs_comp[i] = np.mean(samples[:, 0])\n    sigma_gibbs_comp[i] = np.mean(samples[:, 1])\n\nprint(\"✓ All predictions obtained\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "qk22y30bcc",
   "source": "# Comprehensive comparison plot: Standard NN vs IS NN vs Gibbs\nfig, axes = plt.subplots(2, 3, figsize=(18, 12))\n\n# Row 1: ρ predictions\n# Standard NN - rho\naxes[0, 0].scatter(rho_true_comp, rho_std_comp, alpha=0.5, s=20)\naxes[0, 0].plot([-1, 1], [-1, 1], 'r--', linewidth=2, label='Perfect prediction')\naxes[0, 0].set_xlabel('True ρ')\naxes[0, 0].set_ylabel('Predicted ρ')\naxes[0, 0].set_title(f'Standard NN: ρ\\nMAE={np.abs(rho_std_comp - rho_true_comp).mean():.4f}')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\naxes[0, 0].set_xlim([-1, 1])\naxes[0, 0].set_ylim([-1, 1])\n\n# IS NN - rho\naxes[0, 1].scatter(rho_true_comp, rho_is_comp, alpha=0.5, s=20, color='orange')\naxes[0, 1].plot([-1, 1], [-1, 1], 'r--', linewidth=2, label='Perfect prediction')\naxes[0, 1].set_xlabel('True ρ')\naxes[0, 1].set_ylabel('Predicted ρ')\naxes[0, 1].set_title(f'IS NN: ρ\\nMAE={np.abs(rho_is_comp - rho_true_comp).mean():.4f}')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\naxes[0, 1].set_xlim([-1, 1])\naxes[0, 1].set_ylim([-1, 1])\n\n# Gibbs - rho\naxes[0, 2].scatter(rho_true_comp, rho_gibbs_comp, alpha=0.5, s=20, color='green')\naxes[0, 2].plot([-1, 1], [-1, 1], 'r--', linewidth=2, label='Perfect prediction')\naxes[0, 2].set_xlabel('True ρ')\naxes[0, 2].set_ylabel('Predicted ρ')\naxes[0, 2].set_title(f'Gibbs: ρ\\nMAE={np.abs(rho_gibbs_comp - rho_true_comp).mean():.4f}')\naxes[0, 2].legend()\naxes[0, 2].grid(True, alpha=0.3)\naxes[0, 2].set_xlim([-1, 1])\naxes[0, 2].set_ylim([-1, 1])\n\n# Row 2: σ predictions\n# Standard NN - sigma\naxes[1, 0].scatter(sigma_true_comp, sigma_std_comp, alpha=0.5, s=20)\naxes[1, 0].plot([0, A], [0, A], 'r--', linewidth=2, label='Perfect prediction')\naxes[1, 0].set_xlabel('True σ')\naxes[1, 0].set_ylabel('Predicted σ')\naxes[1, 0].set_title(f'Standard NN: σ\\nMAE={np.abs(sigma_std_comp - sigma_true_comp).mean():.4f}')\naxes[1, 0].legend()\naxes[1, 0].grid(True, alpha=0.3)\naxes[1, 0].set_xlim([0, A])\naxes[1, 0].set_ylim([0, A])\n\n# IS NN - sigma\naxes[1, 1].scatter(sigma_true_comp, sigma_is_comp, alpha=0.5, s=20, color='orange')\naxes[1, 1].plot([0, A], [0, A], 'r--', linewidth=2, label='Perfect prediction')\naxes[1, 1].set_xlabel('True σ')\naxes[1, 1].set_ylabel('Predicted σ')\naxes[1, 1].set_title(f'IS NN: σ\\nMAE={np.abs(sigma_is_comp - sigma_true_comp).mean():.4f}')\naxes[1, 1].legend()\naxes[1, 1].grid(True, alpha=0.3)\naxes[1, 1].set_xlim([0, A])\naxes[1, 1].set_ylim([0, A])\n\n# Gibbs - sigma\naxes[1, 2].scatter(sigma_true_comp, sigma_gibbs_comp, alpha=0.5, s=20, color='green')\naxes[1, 2].plot([0, A], [0, A], 'r--', linewidth=2, label='Perfect prediction')\naxes[1, 2].set_xlabel('True σ')\naxes[1, 2].set_ylabel('Predicted σ')\naxes[1, 2].set_title(f'Gibbs: σ\\nMAE={np.abs(sigma_gibbs_comp - sigma_true_comp).mean():.4f}')\naxes[1, 2].legend()\naxes[1, 2].grid(True, alpha=0.3)\naxes[1, 2].set_xlim([0, A])\naxes[1, 2].set_ylim([0, A])\n\nplt.tight_layout()\nplt.show()\n\n# Print summary statistics\nprint(\"\\n\" + \"=\"*80)\nprint(\"COMPARISON SUMMARY: Standard NN vs IS NN vs Gibbs\")\nprint(\"=\"*80)\nprint(f\"ρ estimation:\")\nprint(f\"  Standard NN - MAE: {np.abs(rho_std_comp - rho_true_comp).mean():.6f}\")\nprint(f\"  IS NN       - MAE: {np.abs(rho_is_comp - rho_true_comp).mean():.6f}\")\nprint(f\"  Gibbs       - MAE: {np.abs(rho_gibbs_comp - rho_true_comp).mean():.6f}\")\nprint(f\"\\nσ estimation:\")\nprint(f\"  Standard NN - MAE: {np.abs(sigma_std_comp - sigma_true_comp).mean():.6f}\")\nprint(f\"  IS NN       - MAE: {np.abs(sigma_is_comp - sigma_true_comp).mean():.6f}\")\nprint(f\"  Gibbs       - MAE: {np.abs(sigma_gibbs_comp - sigma_true_comp).mean():.6f}\")\nprint(\"=\"*80)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}